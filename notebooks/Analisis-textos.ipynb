{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c561be22-73dc-41a2-aa24-8eba8cdf0e83",
   "metadata": {},
   "source": [
    "# Análisis de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2a53c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting num2words\n",
      "  Using cached num2words-0.5.12-py3-none-any.whl (125 kB)\n",
      "Collecting docopt>=0.6.2 (from num2words)\n",
      "  Using cached docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13773 sha256=d570e327a262f955aa9f15a924aa8cdce6e8412719c46b2668d6024e1f374ec2\n",
      "  Stored in directory: c:\\users\\alanf\\appdata\\local\\pip\\cache\\wheels\\fc\\ab\\d4\\5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, num2words\n",
      "Successfully installed docopt-0.6.2 num2words-0.5.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5630835a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alanf\\OneDrive\\Documents\\BI\\Etapa 1\\Proyecto-BI\\notebooks\\Analisis-textos.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alanf/OneDrive/Documents/BI/Etapa%201/Proyecto-BI/notebooks/Analisis-textos.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alanf/OneDrive/Documents/BI/Etapa%201/Proyecto-BI/notebooks/Analisis-textos.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alanf/OneDrive/Documents/BI/Etapa%201/Proyecto-BI/notebooks/Analisis-textos.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas_profiling\u001b[39;00m \u001b[39mimport\u001b[39;00m ProfileReport\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "from matplotlib import pyplot as plt\n",
    "import num2words\n",
    "import re, unicodedata, inflect\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay, RocCurveDisplay,\n",
    "    roc_auc_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f934984",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECALL = \"Recall:\"\n",
    "PRECISION = \"Precision:\"\n",
    "F1 = \"F1:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed92f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring pandas to show all cell content\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading stopwords\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb01c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ods_df = pd.read_csv(\"../data/cat_6716.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ods_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ods_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef070cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ods_df[\"sdg\"].value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc57821",
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = ods_df.copy()\n",
    "textos['Conteo'] = [len(x) for x in textos['Textos_espanol']]\n",
    "textos['Moda'] = [pd.Series(x).value_counts().index[0] for x in textos['Textos_espanol']]\n",
    "textos['Max'] = [[max([len(x) for x in i.split(' ')])][0] for i in textos['Textos_espanol']]\n",
    "textos['Min'] = [[min([len(x) for x in i.split(' ')])][0] for i in textos['Textos_espanol']]\n",
    "\n",
    "# Se realiza un perfilamiento de los datos con la librería pandas profiling\n",
    "ProfileReport(textos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dee57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ods_df[\"Textos_espanol\"] = ods_df[\"Textos_espanol\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de32e53",
   "metadata": {},
   "source": [
    "## Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c2ddb",
   "metadata": {},
   "source": [
    "Para poder realizar el pre-procesamiento de los datos, es recomendable pasar por tres etapas:\n",
    "* Limpieza de los datos.\n",
    "* Tokenización.\n",
    "* Normalización.\n",
    "\n",
    "<span style=\"color:red\">!!! Antes de ver la solución, revisa alternativas para realizar las tres etapas previas.</span>\n",
    "\n",
    "Para mayor información, pueden consultar el [siguiente artículo](https://medium.com/datos-y-ciencia/preprocesamiento-de-datos-de-texto-un-tutorial-en-python-5db5620f1767\n",
    ")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2124a94e",
   "metadata": {},
   "source": [
    "### Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e28f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    return [word.lower() for word in words]\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = num2words.num2words(int(word), lang='es')\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "def preprocessing(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd89cc3",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ods_df[\"Textos_espanol\"] = ods_df[\"Textos_espanol\"].apply(word_tokenize).apply(preprocessing).apply(\" \".join)\n",
    "ods_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ed664",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7203646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = nltk.stem.SnowballStemmer('spanish')\n",
    "    stems = [stemmer.stem(word) for word in words]\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    #Lemmatize for spanish\n",
    "    stemmer = nltk.stem.SnowballStemmer('spanish')\n",
    "    lemmas = [stemmer.stem(word) for word in words]\n",
    "    return lemmas\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems + lemmas\n",
    "\n",
    "ods_df[\"Textos_espanol\"] = ods_df[\"Textos_espanol\"].apply(word_tokenize).apply(stem_and_lemmatize).apply(\" \".join)\n",
    "ods_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da2af6",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The param 'stratify' is useful to guarantee label proportions on train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(ods_df[[\"Textos_espanol\"]], ods_df[\"sdg\"], test_size=0.3, stratify=ods_df[\"sdg\"], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd5193",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = [nltk.word_tokenize(text) for text in X_train]\n",
    "X_test_tokens = [nltk.word_tokenize(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa94489",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0507ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f133b",
   "metadata": {},
   "source": [
    "## Text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb551a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(tokenizer=word_tokenize, stop_words=stop_words, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f17c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow = bow.fit_transform(X_train[\"Textos_espanol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary size:\", len(bow.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03839a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stop_words, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c44fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf.fit_transform(X_train[\"Textos_espanol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d82634",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary size:\", len(tfidf.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f45d192",
   "metadata": {},
   "source": [
    "## Training a model with BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc6201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = RandomForestClassifier(random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472d2551",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model.fit(X_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd311e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance in tree models is an indicator on how relevant is a feature for taking the decision by the model\n",
    "pd.Series(bow_model.feature_importances_, index=bow.vocabulary_).sort_values().tail(20).plot.barh(figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fafaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_estimators = bow_model.estimators_\n",
    "print(\"Number of trees:\", len(bow_estimators))\n",
    "print(\"Trees depth (mean):\", np.mean([tree.get_depth() for tree in bow_estimators]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1eb30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bow_predict = bow_model.predict(X_bow)\n",
    "y_test_bow_predict = bow_model.predict(bow.transform(X_test[\"Textos_espanol\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fefa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_bow_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_bow_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PRECISION, precision_score(y_train, y_train_bow_predict, average=\"weighted\"))\n",
    "print(RECALL, recall_score(y_train, y_train_bow_predict, average=\"weighted\"))\n",
    "print(F1, f1_score(y_train, y_train_bow_predict, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PRECISION, precision_score(y_test, y_test_bow_predict, average=\"weighted\"))\n",
    "print(RECALL, recall_score(y_test, y_test_bow_predict, average=\"weighted\"))\n",
    "print(F1, f1_score(y_test, y_test_bow_predict, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aede2c8",
   "metadata": {},
   "source": [
    "## Training a model with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = RandomForestClassifier(random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118dff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model.fit(X_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c469495",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(tfidf_model.feature_importances_, index=tfidf.vocabulary_).sort_values().tail(20).plot.barh(figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_estimators = tfidf_model.estimators_\n",
    "print(\"Number of trees:\", len(tfidf_estimators))\n",
    "print(\"Trees depth (mean):\", np.mean([tree.get_depth() for tree in tfidf_estimators]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb624253",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tfidf_predict = tfidf_model.predict(X_tfidf)\n",
    "y_test_tfidf_predict = tfidf_model.predict(tfidf.transform(X_test[\"Textos_espanol\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8146fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_test_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PRECISION, precision_score(y_train, y_train_tfidf_predict, average=\"weighted\"))\n",
    "print(RECALL, recall_score(y_train, y_train_tfidf_predict, average=\"weighted\"))\n",
    "print(F1, f1_score(y_train, y_train_tfidf_predict, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PRECISION, precision_score(y_test, y_test_tfidf_predict, average=\"weighted\"))  \n",
    "print(RECALL, recall_score(y_test, y_test_tfidf_predict, average=\"weighted\"))\n",
    "print(F1, f1_score(y_test, y_test_tfidf_predict, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a73b6",
   "metadata": {},
   "source": [
    "## Training a model with Words Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae957d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=1, sg=0, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener el embedding de un texto\n",
    "def text_to_vector(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    vector = [embedding_model.wv[word] for word in words if word in embedding_model.wv]\n",
    "    if not vector:\n",
    "        return [0.0] * embedding_model.vector_size\n",
    "    return sum(vector) / len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir textos a vectores\n",
    "X_train_wv = [text_to_vector(text) for text in X_train]\n",
    "X_test_wv = [text_to_vector(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar y entrenar el clasificador Random Forest\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=2)\n",
    "rf_classifier.fit(X_train_wv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c561de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_test_rf_predict = rf_classifier.predict(X_test_wv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baae47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular métricas de evaluación\n",
    "precision_rf = precision_score(y_test, y_test_rf_predict, average=\"weighted\")\n",
    "recall_rf = recall_score(y_test, y_test_rf_predict, average=\"weighted\")\n",
    "f1_rf = f1_score(y_test, y_test_rf_predict, average=\"weighted\")\n",
    "\n",
    "print(\"Random Forest - Precision:\", precision_rf)\n",
    "print(\"Random Forest - Recall:\", recall_rf)\n",
    "print(\"Random Forest - F1 Score:\", f1_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3415006",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [\n",
    "    (\"vectorizer\", CountVectorizer(tokenizer=word_tokenize, stop_words=stop_words, lowercase=True)),\n",
    "    (\"classifier\", RandomForestClassifier(random_state=4))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eabc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"vectorizer\": [CountVectorizer(tokenizer=word_tokenize, stop_words=stop_words), TfidfVectorizer(tokenizer=word_tokenize, stop_words=stop_words)],\n",
    "    \"vectorizer__lowercase\": [True, False],\n",
    "    \"classifier__n_estimators\": [50, 100],\n",
    "    \"classifier__criterion\": ['gini', 'entropy'],\n",
    "    \"classifier__max_depth\": [25, 50, 75, 100]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a09206",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = RandomizedSearchCV(pipeline, param_grid, n_iter=10, scoring=[\"precision\", \"recall\", \"f1\"], refit=\"f1\", cv=3, return_train_score=True, verbose=1, random_state=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X_train[\"Textos_espanol\"], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d086352",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_search_predict = search.best_estimator_.predict(X_train[\"Textos_espanol\"])\n",
    "y_test_search_predict = search.best_estimator_.predict(X_test[\"Textos_espanol\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231fd7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PRECISION, precision_score(y_train, y_train_search_predict, average=\"weighted\"))\n",
    "print(RECALL, recall_score(y_train, y_train_search_predict, average=\"weighted\"))\n",
    "print(F1, f1_score(y_train, y_train_search_predict, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PRECISION, precision_score(y_test, y_test_search_predict, average=\"weighted\"))\n",
    "print(RECALL, recall_score(y_test, y_test_search_predict, average=\"weighted\"))\n",
    "print(F1, f1_score(y_test, y_test_search_predict, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd93bf",
   "metadata": {},
   "source": [
    "# Predicción y guardado de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbfd899",
   "metadata": {},
   "outputs": [],
   "source": [
    "ods_df_unlabeled = pd.read_csv(\"../data/SinEtiquetatest_cat_6716.csv\")\n",
    "#Predict the labels of the unlabeled data\n",
    "y_unlabeled_predict = search.best_estimator_.predict(ods_df_unlabeled[\"Textos_espanol\"])\n",
    "#Add the predicted labels to the unlabeled data\n",
    "ods_df_unlabeled[\"sdg\"] = y_unlabeled_predict\n",
    "#Save the labeled data\n",
    "ods_df_unlabeled.to_csv(\"../data/Predicted.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
